{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65956df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (131328x3 and 304x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m     79\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m---> 80\u001b[0m     actor_logits, critic_value \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mactor_logits)\n\u001b[0;32m     82\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (131328x3 and 304x256)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from gym_pikachu_volleyball.envs.pikachu_volleyball import PikachuVolleyballMultiEnv\n",
    "\n",
    "# Define the A2C actor and critic networks\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.0001\n",
    "gamma = 0.99\n",
    "num_steps = 5\n",
    "max_episodes = 10000\n",
    "\n",
    "# Create the environment\n",
    "env = PikachuVolleyballMultiEnv(render_mode='human')\n",
    "\n",
    "# Initialize the actor-critic network and optimizer\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = sum(env.action_space.shape)\n",
    "hidden_size = 256\n",
    "model = ActorCritic(num_inputs, num_actions, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define the A2C update function\n",
    "def update(model, optimizer, states, actions, rewards, next_states, masks):\n",
    "    # Preprocess the observation tensor\n",
    "    states = torch.FloatTensor(states).view(-1, 304 * 432 * 3)\n",
    "    next_states = torch.FloatTensor(next_states).view(-1, 304 * 432 * 3)\n",
    "\n",
    "    _, critic_values = model(states)\n",
    "    _, next_critic_values = model(next_states)\n",
    "\n",
    "    td_targets = rewards + gamma * next_critic_values * masks\n",
    "    td_errors = td_targets - critic_values\n",
    "\n",
    "    actor_logits, _ = model(states)\n",
    "    dist = Categorical(logits=actor_logits)\n",
    "    log_probs = dist.log_prob(actions)\n",
    "\n",
    "    actor_loss = -(log_probs * td_errors.detach()).mean()\n",
    "    critic_loss = F.smooth_l1_loss(critic_values, td_targets.detach())\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "     \n",
    "# Train the A2C agent\n",
    "episode_rewards = []\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset(option)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Collect experience by taking num_steps in the environment\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        for _ in range(num_steps):\n",
    "            states.append(state)\n",
    "            actor_logits, critic_value = model(torch.FloatTensor(state))\n",
    "            dist = Categorical(logits=actor_logits)\n",
    "            action = dist.sample().numpy()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            masks.append([1.0] * sum(env.action_space.shape))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        next_states = state\n",
    "\n",
    "        # Update the actor-critic network using the collected experience\n",
    "        update(model, optimizer, torch.FloatTensor(states), torch.FloatTensor(actions),\n",
    "               torch.FloatTensor(rewards), torch.FloatTensor(next_states), torch.FloatTensor(masks))\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('Episode %d | Reward: %d' % (episode, episode_reward))\n",
    "\n",
    "    # Save the model weights every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        torch.save(model.state_dict(), 'a2c_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56655aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
