{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bc0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym_pikachu_volleyball\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from itertools import count\n",
    "from collections import deque, namedtuple\n",
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ff8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory :\n",
    "    def __init__(self, capacity=10000) :\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    def push(self, trans) :\n",
    "        self.memory.append(trans)\n",
    "    def sample(self, num) :\n",
    "        return random.sample(self.memory, num)\n",
    "    def __len__(self) :\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310af70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_net(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc_actor = nn.Linear(1120, 256)\n",
    "        self.out_actor = nn.Linear(256, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, 1120)\n",
    "        actor_out = nn.functional.relu(self.fc_actor(x))\n",
    "        actor_out = self.out_actor(actor_out)\n",
    "        return actor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e7112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent :\n",
    "    def __init__(self, device, Transition, BATCH=32, GAMMA=0.99, TAU=0.005, LR=1e-4,\n",
    "                EPS_START=0.9, EPS_END=0.05, EPS_DECAY=1000, capacity=100000) :\n",
    "        self.device = device\n",
    "        self.Transition = Transition\n",
    "        self.BATCH = BATCH\n",
    "        self.GAMMA = GAMMA\n",
    "        self.TAU = TAU\n",
    "        self.LR = LR\n",
    "        self.EPS_START = EPS_START\n",
    "        self.EPS_END = EPS_END\n",
    "        self.EPS_DECAY = EPS_DECAY\n",
    "        self.memory = Memory(capacity)\n",
    "        self.n_observations = 4\n",
    "        self.n_actions = 2\n",
    "        self.time_step = 0\n",
    "        self.policy_net = Q_net().to(device)\n",
    "        #self.policy_net.apply(self.init_weights)\n",
    "        self.target_net = Q_net().to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(0)\n",
    "    def select_action(self, state) :\n",
    "        self.time_step += 1\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1 * self.time_step / self.EPS_DECAY)\n",
    "        if sample > eps_threshold :\n",
    "            with torch.no_grad() :\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else :\n",
    "            return torch.tensor([[random.randrange(0,2)]], device=self.device, dtype=torch.long)\n",
    "    def optimize(self) :\n",
    "        if len(self.memory) < self.BATCH * 100:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.BATCH)\n",
    "        batch = self.Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        #print(state_batch.shape)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.BATCH, device=self.device)\n",
    "        with torch.no_grad() :\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    def update(self) :\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4adcf5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color(s) :\n",
    "    nonblack = (s != [0, 0, 0]).any(axis=2)\n",
    "    s[nonblack] = [255, 255, 255]\n",
    "    result_img = Image.fromarray(s)\n",
    "    result_img = np.array(result_img)\n",
    "    result_img = 255 - result_img\n",
    "    result_img = Image.fromarray(result_img)\n",
    "    result_img = transform(result_img)\n",
    "    #result_img = result_img / 255.0\n",
    "    return result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16987f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 episode : 22\n",
      "2번째 episode : 22\n",
      "3번째 episode : 22\n",
      "4번째 episode : 22\n",
      "5번째 episode : 22\n",
      "6번째 episode : 22\n",
      "7번째 episode : 22\n",
      "8번째 episode : 22\n",
      "9번째 episode : 22\n",
      "10번째 episode : 22\n",
      "11번째 episode : 22\n",
      "12번째 episode : 22\n",
      "13번째 episode : 22\n",
      "14번째 episode : 22\n",
      "15번째 episode : 22\n",
      "16번째 episode : 22\n",
      "17번째 episode : 22\n",
      "18번째 episode : 22\n",
      "19번째 episode : 22\n",
      "20번째 episode : 22\n",
      "21번째 episode : 22\n",
      "22번째 episode : 22\n",
      "23번째 episode : 22\n",
      "24번째 episode : 22\n",
      "25번째 episode : 22\n",
      "26번째 episode : 22\n",
      "27번째 episode : 22\n",
      "28번째 episode : 22\n",
      "29번째 episode : 22\n",
      "30번째 episode : 22\n",
      "31번째 episode : 22\n",
      "32번째 episode : 22\n",
      "33번째 episode : 22\n",
      "34번째 episode : 22\n",
      "35번째 episode : 22\n",
      "36번째 episode : 22\n",
      "37번째 episode : 22\n",
      "38번째 episode : 22\n",
      "39번째 episode : 22\n",
      "40번째 episode : 22\n",
      "41번째 episode : 22\n",
      "42번째 episode : 22\n",
      "43번째 episode : 22\n",
      "44번째 episode : 22\n",
      "45번째 episode : 22\n",
      "46번째 episode : 22\n",
      "47번째 episode : 22\n",
      "48번째 episode : 22\n",
      "49번째 episode : 22\n",
      "50번째 episode : 22\n",
      "51번째 episode : 22\n",
      "52번째 episode : 22\n",
      "53번째 episode : 22\n",
      "54번째 episode : 22\n",
      "55번째 episode : 22\n",
      "56번째 episode : 22\n",
      "57번째 episode : 22\n",
      "58번째 episode : 22\n",
      "59번째 episode : 22\n",
      "60번째 episode : 22\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "env = gym.make(\"PikachuVolleyball-v0\", render_mode = None)\n",
    "option={'is_player1_serve' : True, 'is_player2_serve' : True}\n",
    "device = torch.device('cpu')\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                       ('state', 'action', 'next_state', 'reward'))\n",
    "dqn_agent_1 = Agent(device, Transition)\n",
    "dqn_agent_2 = Agent(device, Transition)\n",
    "\n",
    "num_episode = 1000\n",
    "scores = []\n",
    "\n",
    "for i_episode in range(num_episode) :\n",
    "    state = env.reset(options=option)\n",
    "    state = convert_color(state).unsqueeze(0)\n",
    "    i_episode_score = 0\n",
    "    for t in count() :\n",
    "        i_episode_score += 1\n",
    "        action_1 = dqn_agent_1.select_action(state)\n",
    "        action_2 = dqn_agent_2.select_action(state)\n",
    "        observation, reward, terminated, _ = env.step([action_1.item(), action_2.item()])\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        reward = torch.tensor([1], device=device)\n",
    "        done = terminated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state =  convert_color(observation).unsqueeze(0)\n",
    "        dqn_agent_1.memory.push(Transition(state, action_1, next_state, reward))\n",
    "        dqn_agent_2.memory.push(Transition(state, action_2, next_state, reward))\n",
    "        state = next_state\n",
    "        dqn_agent_1.optimize()\n",
    "        dqn_agent_1.update()\n",
    "        dqn_agent_2.optimize()\n",
    "        dqn_agent_2.update()\n",
    "\n",
    "        if done :\n",
    "            print(str(i_episode+1) + \"번째 episode : \" + str(i_episode_score))\n",
    "            scores.append(i_episode_score)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a18b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
